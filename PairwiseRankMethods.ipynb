{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Learning To Rank Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "#from tf_utils import load_dataset, random_mini_batches, predict\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import h5py\n",
    "#import numpy as np\n",
    "#import tensorflow as tf\n",
    "#import math\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 62, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    return mini_batches\n",
    "\n",
    "def predict(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [X.shape[0], 1])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.round(tf.sigmoid(z3))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "def predict_proba(X, parameters):\n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [X.shape[0], X.shape[1]])\n",
    "    \n",
    "    z3 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.sigmoid(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3'] \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def load_XY():\n",
    "    dataset = pd.read_csv('D:/GHDataset/dataset/splits/dataSetPropuestaCSVTrainEval')\n",
    "    #dataset = pd.read_csv('D:/GHDataset/Dataset/dataSetPropuestaCSVMedium')\n",
    "    #dataset = pd.read_csv('D:/GHDataset/Dataset/dataSetPropuestaCSVLarge')\n",
    "    X = dataset.loc[:, dataset.columns != 'label']\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    X = X.fillna(0)\n",
    "    y = dataset['label']\n",
    "    \n",
    "    return X.as_matrix().transpose(),y.values.reshape((1, y.shape[0]))\n",
    "\n",
    "def load_data_normal(file='D:/GHDataset/dataset/splits/dataSetPropuestaCSVTrainEval'):\n",
    "    dataset = pd.read_csv(file)\n",
    "    #dataset = pd.read_csv('D:/GHDataset/Dataset/dataSetPropuestaCSVMedium')\n",
    "    #dataset = pd.read_csv('D:/GHDataset/Dataset/dataSetPropuestaCSVLarge')\n",
    "    X = dataset.loc[:, dataset.columns != 'label']\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    X = X.fillna(0)\n",
    "    y = dataset['label']\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "def load_XY_shuffle(random_state=0, file='D:/GHDataset/dataset/splits/dataSetPropuestaCSVTrainEval'):\n",
    "    dataset = pd.read_csv(file)\n",
    "    dataset = dataset.sample(random_state=random_state,frac=1).reset_index(drop=True)\n",
    "    X = dataset.loc[:, dataset.columns != 'label']\n",
    "    #X = dataset[['item1f7','item1f14','item1f24','item2f7','item2f14','item2f24']]\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    X = X.fillna(0)\n",
    "    y = dataset['label']\n",
    "    \n",
    "    return X.as_matrix().transpose(),y.values.reshape((1, y.shape[0]))\n",
    "\n",
    "def load_technology_dataset(test_size=0.25, random_state=0, file='D:/GHDataset/datasets/splits/dataSetPropuestaCSVTrainEval'):\n",
    "    dataset = pd.read_csv(file)\n",
    "    dataset = dataset.sample(random_state=0,frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X = dataset.loc[:, dataset.columns != 'label']\n",
    "    \n",
    "    #print(X.head(20))\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    X = X.fillna(0)\n",
    "    #print(X.head(20))\n",
    "    y = dataset[['label']]\n",
    "    #y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "\n",
    "    train_set_x_orig, test_set_x_orig, train_set_y_orig, test_set_y_orig = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    #print(train_set_y_orig.shape)\n",
    "    \n",
    "    #train_set_y_orig = train_set_y_orig.as_matrix().transpose()\n",
    "    #test_set_y_orig = test_set_y_orig.as_matrix().transpose()\n",
    "    train_set_y_orig = train_set_y_orig.values.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.values.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig.as_matrix().transpose(), train_set_y_orig, test_set_x_orig.as_matrix().transpose(), test_set_y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train, Y_train, X_test, Y_test = load_technology_dataset(0.25,0)\n",
    "print(X_train.shape,X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model** is *LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_x, n_y, ):\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "print(\"X = \" + str(X))\n",
    "print(\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(X,Y):\n",
    "    \n",
    "    tf.set_random_seed(1)  \n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [50, X.shape[0]], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable(\"b1\", [50, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [25, 50], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable(\"b2\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [1, 25], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable(\"b3\", [Y.shape[0], 1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(X,Y)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, keep_prob=1):\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    drop_outA1 = tf.nn.dropout(A1, keep_prob)\n",
    "    Z2 = tf.add(tf.matmul(W2, drop_outA1), b2)             # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    drop_outA2 = tf.nn.dropout(A2, keep_prob)\n",
    "    Z3 = tf.add(tf.matmul(W3, drop_outA2), b3)             # Z3 = np.dot(W3,Z2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_parameters(X,Y)\n",
    "    Z3 = forward_propagation(X, parameters,1)\n",
    "    print(\"Z3 = \" + str(Z3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y, parameters, beta=0.0):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    regularizer = tf.nn.l2_loss(parameters['W1']) + tf.nn.l2_loss(parameters['W2']) + tf.nn.l2_loss(parameters['W3']) + tf.nn.l2_loss(parameters['b1']) + tf.nn.l2_loss(parameters['b2']) + tf.nn.l2_loss(parameters['b3'])\n",
    "    \n",
    "    #vars = tf.trainable_variables() \n",
    "    #regularizer = tf.add_n([ tf.nn.l2_loss(v) for v in vars ])\n",
    "\n",
    "    cost = tf.reduce_mean(cost + beta * regularizer)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters = initialize_parameters(X, Y)\n",
    "    Z3 = forward_propagation(X, parameters,1)\n",
    "    cost = compute_cost(Z3, Y, parameters,0)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.00004,\n",
    "          num_epochs = 500, minibatch_size = 35, print_cost = False, keep_prob=0.80, beta=0.005,\n",
    "          optimizer='gradient'):\n",
    "\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(X, Y)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters, keep_prob)\n",
    "   \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y, parameters, beta)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    if(optimizer =='adam'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    elif(optimizer =='gradient'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    elif(optimizer =='momentum'):\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                #print(minibatch_cost,num_minibatches)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 200 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 50 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        \"\"\"plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        #print(\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        #err = tf.constant(0.01, name='err') \n",
    "        correct_prediction = tf.equal(tf.round(tf.sigmoid(Z3)), tf.round(Y))\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        acc_train = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        acc_test = accuracy.eval({X: X_test, Y: Y_test})\n",
    "       # print(\"Train Accuracy:\", acc_train)\n",
    "       # print(\"Test Accuracy:\", acc_test)\n",
    "        \n",
    "        return parameters, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_datasets = ['datasets/features/pairwise_values/train-eval/dataSetPropuestaCSVOnlyFrontTrainEval',\n",
    "                  'datasets/features/pairwise_values/train-eval/dataSetPropuestaCSVOnlyBackTrainEval', \n",
    "                  'datasets/features/pairwise_values/train-eval/dataSetPropuestaCSVFrontTrainEval',\n",
    "                  'datasets/features/pairwise_values/train-eval/dataSetPropuestaCSVBackTrainEval', \n",
    "                  'datasets/features/pairwise_values/train-eval/dataSetPropuestaCSVTrainEval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cross Validation RankNet\n",
    "\n",
    "\n",
    "for file in array_datasets:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Dataset: \" + file)\n",
    "    print()\n",
    "\n",
    "    X, Y = load_XY_shuffle(random_state=100, file=file)\n",
    "    n_splits=5\n",
    "    k_fold = KFold(n_splits=n_splits)\n",
    "\n",
    "    #print(X.shape, Y.shape)\n",
    "\n",
    "    count = 0\n",
    "    #optimizers = ['gradient','adam']\n",
    "    beta = 0.005\n",
    "    optimizer = 'adam'\n",
    "    keep_prob = 0.80\n",
    "\n",
    "    acc_train_sum = 0\n",
    "    acc_test_sum = 0\n",
    "    for train_indices, test_indices in k_fold.split(X.transpose()):\n",
    "        count = count +1;\n",
    "        #print(\"KFold number \" + str(count) )\n",
    "\n",
    "        xtrain = X[:,train_indices.tolist()]\n",
    "        ytrain = Y[:,train_indices.tolist()]\n",
    "        xtest = X[:,test_indices.tolist()]\n",
    "        ytest = Y[:,test_indices.tolist()]\n",
    "        minibatch_size=(int)(np.floor(train_indices.size/(n_splits*2)))\n",
    "\n",
    "        parameters, acc_train, acc_test = model(xtrain, ytrain, xtest, ytest, learning_rate = 0.00004,\n",
    "                                                minibatch_size=minibatch_size,keep_prob=keep_prob,\n",
    "                                                beta=beta, num_epochs = 7000, optimizer=optimizer)\n",
    "        acc_train_sum = acc_train_sum + acc_train\n",
    "        acc_test_sum = acc_test_sum + acc_test\n",
    "    print(\"Train Accuracy KFold:\", acc_train_sum/n_splits)\n",
    "    print(\"Test Accuracy KFold:\", acc_test_sum/n_splits)\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rank Net**\n",
    "\n",
    "**Best parameters set found on development set:**\n",
    "\n",
    "* learning_rate = 0.00004\n",
    "* keep_prob=0.80 \n",
    "* beta=4e-05  \n",
    "* epoch=7000\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Accuracy/DataSet**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Only Front**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Only Back**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Front**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Back**\n",
    "        </td>\n",
    "        <td>\n",
    "            **ALL**\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Train Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.9926710009574891\n",
    "        </td>\n",
    "        <td>\n",
    "            0.9378761649131775\n",
    "        </td>\n",
    "        <td>\n",
    "            0.9655063390731812\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8848104119300843\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8794930934906006\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Test Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8379211902618409\n",
    "        </td>\n",
    "        <td>\n",
    "            0.7746713280677795\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8417721629142761\n",
    "        </td>\n",
    "        <td>\n",
    "            0.7934044241905213\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8018433213233948\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prediction RankNet\n",
    "\n",
    "array_parameters = []\n",
    "\n",
    "for file in array_datasets:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Dataset: \" + file)\n",
    "    print()\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test = load_technology_dataset(0.25,0,file=file)\n",
    "\n",
    "    beta = 0.005\n",
    "    optimizer = 'adam'\n",
    "    keep_prob = 0.80\n",
    "\n",
    "    parameters, acc_train, acc_test = model(X_train, Y_train, X_test, Y_test, keep_prob=keep_prob,\n",
    "                                            beta=beta, num_epochs = 7000, optimizer=optimizer)\n",
    "    array_parameters.append(parameters)\n",
    "    \n",
    "    print(\"Train Accuracy KFold:\", acc_train)\n",
    "    print(\"Test Accuracy KFold:\", acc_test)\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'RankNet_parameters'\n",
    "pickle.dump(array_parameters, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the models from disk\n",
    "loaded_models = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "print(X_train[:,0:1].shape)\n",
    "print(predict(X_train[:,0:1], loaded_models[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### GRADIENT BOOST ###\n",
    "### AKA GBRank ###\n",
    "np.random.seed(0)\n",
    "\n",
    "feature_importances = []\n",
    "\n",
    "GBRank_models = []\n",
    "\n",
    "for file in array_datasets:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Dataset: \" + file)\n",
    "    print()\n",
    "\n",
    "    X, y = load_data_normal(file=file)\n",
    "\n",
    "    X = X.as_matrix()\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100,test_size=0.20)\n",
    "    print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "    #tuned_parameters = [{'n_neighbors': [10,20,30,40,50,65,100,105,110,120]}]\n",
    "\n",
    "    scores = ['accuracy']\n",
    "\n",
    "\n",
    "    '''print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()'''\n",
    "\n",
    "    #param_test1 = {'n_estimators':[4500],'learning_rate':[0.004], 'max_depth':[50],\n",
    "    #               'min_samples_split':[50], 'min_samples_leaf':[10]}\n",
    "    #param_test1 = {'n_estimators':[3000],'learning_rate':[0.005], 'max_depth':[50],\n",
    "    #               'min_samples_split':[50], 'min_samples_leaf':[10]}\n",
    "    param_test1 = {'n_estimators':[2375],'learning_rate':[0.004], 'max_depth':[50],\n",
    "                   'min_samples_split':[50], 'min_samples_leaf':[10]}\n",
    "    clf = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.005, \n",
    "                                                              min_samples_split=50,min_samples_leaf=10,\n",
    "                                                              max_depth=50,max_features='log2',subsample=0.8,random_state=200),  \n",
    "                                    param_grid = param_test1, scoring='accuracy',n_jobs=4,iid=False, cv=5)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    '''print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.5f (+/-%0.5f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "'''\n",
    "    #print(\"Detailed classification report:\")\n",
    "    #print()\n",
    "    #print(\"The model is trained on the full development set.\")\n",
    "    #print(\"The scores are computed on the full evaluation set.\")\n",
    "    \n",
    "    GBRank_models.append(clf)\n",
    "    \n",
    "    y_true, y_pred = y_train, clf.predict(X_train)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(\"Training Accuracy - KFold\")\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    #print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(\"Test Accuracy - KFold\")\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(\"Feature_Importances_ \")\n",
    "    #print(clf.best_estimator_.feature_importances_)\n",
    "    feature_importances.append(clf.best_estimator_.feature_importances_)\n",
    "    #print()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientBoostingClassifier**\n",
    "\n",
    "**Best parameters set found on development set:**\n",
    "\n",
    "{'min_samples_leaf': 10, 'min_samples_split': 50, 'max_depth': 50, 'learning_rate': 0.005, 'n_estimators': 3000}\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Accuracy/DataSet**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Only Front**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Only Back**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Front**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Back**\n",
    "        </td>\n",
    "        <td>\n",
    "            **ALL**\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Train Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "        <td>\n",
    "            1\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Test Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8906250000000000\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8554216867469879\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8787878787878788\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8595041322314050\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8747697974217311\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBRank save model\n",
    "import pickle\n",
    "\n",
    "filename = 'GBRank_models'\n",
    "pickle.dump(GBRank_models, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the models from disk\n",
    "loaded_models = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_models[0].predict(X_test)\n",
    "print(X_test.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear SVC ###\n",
    "### AKA RankSVM ###\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "RankSVM_Linear_models = []\n",
    "\n",
    "for file in array_datasets:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Dataset: \" + file)\n",
    "    print()\n",
    "\n",
    "    X, y = load_data_normal(file=file)\n",
    "\n",
    "    X = X.as_matrix()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "    #tuned_parameters = [{'n_neighbors': [10,20,30,40,50,65,100,105,110,120]}]\n",
    "\n",
    "    scores = ['accuracy']\n",
    "\n",
    "    param_test1 = {'C': [5,10,20,30,40,50,70,90,150]}\n",
    "\n",
    "    clf = GridSearchCV(estimator = LinearSVC(max_iter=10000) , param_grid = param_test1, cv=5)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    RankSVM_Linear_models.append(clf)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "\n",
    "    y_true, y_pred = y_train, clf.predict(X_train)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(\"Training Accuracy - KFold\")\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(\"Test Accuracy - KFold\")\n",
    "    print(accuracy_score(y_true, y_pred))                   \n",
    "\n",
    "    print()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinearSVC**\n",
    "\n",
    "**Best parameters set found on development set:**\n",
    "\n",
    "{'C': 10}\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Accuracy/DataSet**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Only Front**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Only Back**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Front**\n",
    "        </td>\n",
    "        <td>\n",
    "            **Back**\n",
    "        </td>\n",
    "        <td>\n",
    "            **ALL**\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Train Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8932291666666666\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8375838926174497\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8597972972972973\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8328741965105602\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8236017209588199\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr> \n",
    "        <td>\n",
    "            **Test Accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8125\n",
    "        </td>\n",
    "        <td>\n",
    "            0.7670682730923695\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8333333333333334\n",
    "        </td>\n",
    "        <td>\n",
    "            0.8071625344352618\n",
    "        </td>\n",
    "        <td>\n",
    "            0.7974217311233885\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankSVM save model\n",
    "\n",
    "filename = 'RankSVM_Linear_models'\n",
    "pickle.dump(RankSVM_Linear_models, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the models from disk\n",
    "loaded_models = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_models[0].predict(X_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AdaBoostClassifier ###\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RankBoost_ada_randomforest_models=[]\n",
    "\n",
    "for file in array_datasets:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Dataset: \" + file)\n",
    "    print()\n",
    "\n",
    "    X, y = load_data_normal(file=file)\n",
    "\n",
    "    X = X.as_matrix()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\n",
    "\n",
    "\n",
    "    scores = ['accuracy']\n",
    "    \n",
    "    param_test1 = {'n_estimators':[3500],'learning_rate':[0.006]}\n",
    "    \n",
    "    #estimator=RandomForestClassifier(random_state=200)\n",
    "    clf = GridSearchCV(estimator = AdaBoostClassifier(random_state=200,n_estimators=1000,learning_rate=0.005),  \n",
    "                                    param_grid = param_test1, scoring='accuracy',\n",
    "                       n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    RankBoost_ada_randomforest_models.append(clf)\n",
    "    \n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "\n",
    "    y_true, y_pred = y_train, clf.predict(X_train)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(\"Training Accuracy - KFold\")\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "    print(\"Test Accuracy - KFold\")\n",
    "    print(accuracy_score(y_true, y_pred))                   \n",
    "\n",
    "    print()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankBoost ADA save model\n",
    "import pickle\n",
    "\n",
    "filename = 'RankBoost_models'\n",
    "pickle.dump(RankBoost_ada_randomforest_models, open(filename, 'wb'))\n",
    "\n",
    "# some time later...\n",
    "\n",
    "# load the models from disk\n",
    "loaded_models = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_models[0].predict(X)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBRank save model\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "configurations = ['OnlyWeb', \n",
    "                  'OnlyNode',\n",
    "                  'Web', \n",
    "                  'Node',\n",
    "                  'All']\n",
    "\n",
    "stored_models = ['GBRank_models','RankSVM_Linear_models', 'RankBoost_models', 'RankNet_parameters']\n",
    "\n",
    "for filename in stored_models:\n",
    "\n",
    "    loaded_models = pickle.load(open(\"Models/\"+filename, 'rb'))\n",
    "\n",
    "    tprs = []\n",
    "    base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    for i in range(0,len(array_datasets)):\n",
    "\n",
    "        file=array_datasets[i]\n",
    "\n",
    "    #    print(\"Dataset: \" + file)\n",
    "    #    print()\n",
    "\n",
    "        X, y = load_data_normal(file=file)\n",
    "\n",
    "        X = X.as_matrix()\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100,test_size=0.20)\n",
    "    #    print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "        \n",
    "        if(filename == 'RankSVM_Linear_models'):\n",
    "            clf = CalibratedClassifierCV(loaded_models[i])\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_score = clf.predict_proba(X_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score[:, 1])\n",
    "\n",
    "        elif(filename == 'RankNet_parameters'):\n",
    "            y_score = predict_proba(X_test.T, loaded_models[i])\n",
    "            y_score = y_score.T\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "            \n",
    "        else:\n",
    "            y_score = loaded_models[i].predict_proba(X_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score[:, 1])\n",
    "             \n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.5, label='ROC %s (AUC = %0.2f)' % (configurations[i], roc_auc))\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    "\n",
    "\n",
    "    tprs = np.array(tprs)\n",
    "    mean_tprs = tprs.mean(axis=0)\n",
    "    std = tprs.std(axis=0)\n",
    "\n",
    "    tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "    tprs_lower = mean_tprs - std\n",
    "\n",
    "\n",
    "    plt.plot(base_fpr, mean_tprs, 'b')\n",
    "    plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='grey', alpha=0.3, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1],'r--', label='Random')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.axes().set_aspect('equal', 'datalim')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('ROC-AUC in pairwise ranking ' + filename.split('_')[0])\n",
    "    plt.savefig('ROC' + filename.split('_')[0] + '.pdf' , bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "BFd89",
   "launcher_item_id": "AH2rK"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
